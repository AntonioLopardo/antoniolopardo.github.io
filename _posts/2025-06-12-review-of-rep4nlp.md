---
layout: post
title: "What's Happening in Representation Learning? A Look at REP4NLP 2025"
date: 2025-06-12 00:00:00 +0000
categories: nlp
# Optional: add or remove tags as you prefer
tags: [Research, NLP]
published: false
---

## Why I Care About Representation Learning

[PLACEHOLDER - Personal introduction and motivation paragraph]

[PLACEHOLDER - Context and background paragraph with TODO notes]

[PLACEHOLDER - Caveats and methodology paragraph]

## REP4NLP 2025: What the Community is Working On

[PLACEHOLDER - Introduction to paper categories]

### 🔬 Interpretability and Understanding Model Representations

**Tracking Universal Features Through Fine-Tuning and Model Merging**  
📊 **OpenReview:** N/A | **Suggestion:** [Analyze (Standout)](#tracking-universal-features-through-fine-tuning-and-model-merging) | 🔗 [ACL Anthology](https://aclanthology.org/2025.repl4nlp-1.1/)  
<small>🏷️ **Tags:** #feature-analysis #model-merging #sparse-autoencoders #transfer-learning</small>

**A Comparative Study of Learning Paradigms in Large Language Models via Intrinsic Dimension**  
📊 **OpenReview:** N/A | **Suggestion:** [Analyze (Standout)](#a-comparative-study-of-learning-paradigms-in-large-language-models-via-intrinsic-dimension) | 🔗 [ACL Anthology](https://aclanthology.org/2025.repl4nlp-1.2/)  
<small>🏷️ **Tags:** #intrinsic-dimension #in-context-learning #supervised-fine-tuning #representation-analysis</small>

**Reverse Probing: Evaluating Knowledge Transfer via Finetuned Task Embeddings for Coreference Resolution**  
📊 **OpenReview:** N/A | **Suggestion:** Open | 🔗 [ACL Anthology](https://aclanthology.org/2025.repl4nlp-1.3/)  
<small>🏷️ **Tags:** #probing #knowledge-transfer #coreference-resolution #evaluation-methodology</small>  

**Notes:**
[PLACEHOLDER - Analysis of methodological approach and findings]

### 📝 Text Embeddings

**Prompt Tuning Can Simply Adapt Large Language Models to Text Encoders**  
📊 **OpenReview:** N/A | **Suggestion:** Read (If you like embeedings) | 🔗 [ACL Anthology](https://aclanthology.org/2025.repl4nlp-1.12/)  
<small>🏷️ **Tags:** #prompt-tuning #text-encoders #parameter-efficiency #llm-adaptation</small>

**Notes:**
[PLACEHOLDER - Technical comparison and observations]

**Large Language Models Are Overparameterized Text Encoders**  
📊 **OpenReview:** N/A | **Suggestion:** Read (If you like embeedings) | 🔗 [ACL Anthology](https://aclanthology.org/2025.repl4nlp-1.7/)  
<small>🏷️ **Tags:** #model-pruning #overparameterization #text-encoders #efficiency</small>

**Notes:**
[PLACEHOLDER - Findings about parameter pruning and efficiency questions]

### 🏗️ Alternative Architectures & Pre-training Objectives

**State Space Models are Strong Text Rerankers**  
📊 **OpenReview:** N/A | **Suggestion:** Skim | 🔗 [ACL Anthology](https://aclanthology.org/2025.repl4nlp-1.13/)  
<small>🏷️ **Tags:** #state-space-models #mamba #text-reranking #information-retrieval</small>  

**Notes:**
[PLACEHOLDER - Analysis of experimental dimensions and architecture comparisons]

**Punctuation Restoration Improves Structure Understanding without Supervision**  
📊 **OpenReview:** N/A | **Suggestion:** Skim | 🔗 [ACL Anthology](https://aclanthology.org/2025.repl4nlp-1.14/)  
<small>🏷️ **Tags:** #punctuation-restoration #structural-understanding #unsupervised-learning #linguistic-structure</small>  

**Notes:**
[PLACEHOLDER - Discussion of core idea and experimental results]

**DEPTH: Discourse Education through Pre-Training Hierarchically**  
📊 **OpenReview:** N/A | **Suggestion:** [Read (Standout)](#depth-discourse-education-through-pre-training-hierarchically) | 🔗 [ACL Anthology](https://aclanthology.org/2025.repl4nlp-1.10/)  
<small>🏷️ **Tags:** #discourse-learning #hierarchical-pretraining #educational-nlp</small>

### ⚡ Efficiency Gains

**Choose Your Words Wisely: Domain-adaptive Masking Makes Language Models Learn Faster**  
📊 **OpenReview:** N/A | **Suggestion:** Open | 🔗 [ACL Anthology](https://aclanthology.org/2025.repl4nlp-1.5/)  
<small>🏷️ **Tags:** #domain-adaptation #efficient-training #masked-language-modeling #biomedical-nlp</small>

**Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models**  
📊 **OpenReview:** N/A | **Suggestion:** Open | 🔗 [ACL Anthology](https://aclanthology.org/2025.repl4nlp-1.4/)  
<small>🏷️ **Tags:** #pre-training #fine-tuning #continual-learning #model-analysis</small>

**Vocabulary-level Memory Efficiency for Language Model Fine-tuning**  
📊 **OpenReview:** N/A | **Suggestion:** Open | 🔗 [ACL Anthology](https://aclanthology.org/2025.repl4nlp-1.8/)  
<small>🏷️ **Tags:** #memory-efficiency #vocabulary-optimization #fine-tuning #resource-optimization</small>

### 🧠 Multi-Modal or Task-specific

**Cross-Modal Learning for Music-to-Music-Video Description Generation**  
📊 **OpenReview:** N/A | **Suggestion:** Open | 🔗 [ACL Anthology](https://aclanthology.org/2025.repl4nlp-1.11/)  
<small>🏷️ **Tags:** #cross-modal #music-video #multimodal-learning #generation</small>

**Efficient Document-level Event Relation Extraction**  
📊 **OpenReview:** N/A | **Suggestion:** Open | 🔗 [ACL Anthology](https://aclanthology.org/2025.repl4nlp-1.9/)  
<small>🏷️ **Tags:** #event-extraction #efficiency #document-level #two-stage-framework</small>

**Investigating Adapters for Parameter-efficient Low-resource Automatic Speech Recognition**  
📊 **OpenReview:** N/A | **Suggestion:** Open | 🔗 [ACL Anthology](https://aclanthology.org/2025.repl4nlp-1.6/)  
<small>🏷️ **Tags:** #adapters #parameter-efficiency #speech-recognition #low-resource</small>

## Standout Papers

### Tracking Universal Features Through Fine-Tuning and Model Merging

*Niels Nielsen Horn, Desmond Elliott*

#### My thoughts

[PLACEHOLDER - Analysis of SAE application and mechanistic interpretability implications]

### DEPTH: Discourse Education through Pre-Trained Hierarchically

*Zachary Elisha Bamberger, Ofek Glick, Chaim Baskin, Yonatan Belinkov*

#### My thoughts

[PLACEHOLDER - Technical analysis of discourse understanding approach and findings]

### A Comparative Study of Learning Paradigms in Large Language Models via Intrinsic Dimension

*Saahith Janapati, Yangfeng Ji*

#### My thoughts

[PLACEHOLDER - Discussion of intrinsic dimension concept and learning paradigm comparisons]

### Bonus: From Tokens to Thoughts - How LLMs and Humans Trade Compression for Meaning

*Chen Shani, Dan Jurafsky, Yann LeCun, Ravid Shwartz-Ziv*

[PLACEHOLDER - Detailed analysis of Rate-Distortion Theory application and experimental findings]

#### My thoughts

[PLACEHOLDER - Commentary on Information Bottleneck theory application and implications]

## Next Steps

## Caveats and Limitations
