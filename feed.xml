<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://antoniolopardo.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://antoniolopardo.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-12T21:46:30+00:00</updated><id>https://antoniolopardo.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">What’s Happening in Representation Learning? A Look at REP4NLP 2025</title><link href="https://antoniolopardo.github.io/blog/2025/review-of-rep4nlp/" rel="alternate" type="text/html" title="What’s Happening in Representation Learning? A Look at REP4NLP 2025"/><published>2025-01-27T00:00:00+00:00</published><updated>2025-01-27T00:00:00+00:00</updated><id>https://antoniolopardo.github.io/blog/2025/review-of-rep4nlp</id><content type="html" xml:base="https://antoniolopardo.github.io/blog/2025/review-of-rep4nlp/"><![CDATA[<h2 id="why-i-care-about-representation-learning">Why I Care About Representation Learning</h2> <p>[PLACEHOLDER - Personal introduction and motivation paragraph]</p> <p>[PLACEHOLDER - Context and background paragraph with TODO notes]</p> <p>[PLACEHOLDER - Caveats and methodology paragraph]</p> <h2 id="rep4nlp-2025-what-the-community-is-working-on">REP4NLP 2025: What the Community is Working On</h2> <p>[PLACEHOLDER - Introduction to paper categories]</p> <h3 id="interpretability-and-understanding-model-representations">Interpretability and Understanding Model Representations</h3> <p><strong>Tracking Universal Features Through Fine-Tuning and Model Merging</strong> 📊 <strong>OpenReview:</strong> N/A | <strong>Suggestion:</strong> <a href="#tracking-universal-features-through-fine-tuning-and-model-merging">Analyze (Standout)</a> | 🔗 <a href="https://aclanthology.org/2025.repl4nlp-1.1/">ACL Anthology</a> <small>🏷️ <strong>Tags:</strong> #feature-analysis #model-merging #sparse-autoencoders #transfer-learning</small></p> <p><strong>A Comparative Study of Learning Paradigms in Large Language Models via Intrinsic Dimension</strong> 📊 <strong>OpenReview:</strong> N/A | <strong>Suggestion:</strong> <a href="#a-comparative-study-of-learning-paradigms-in-large-language-models-via-intrinsic-dimension">Analyze (Standout)</a> | 🔗 <a href="https://aclanthology.org/2025.repl4nlp-1.2/">ACL Anthology</a> <small>🏷️ <strong>Tags:</strong> #intrinsic-dimension #in-context-learning #supervised-fine-tuning #representation-analysis</small></p> <p><strong>Reverse Probing: Evaluating Knowledge Transfer via Finetuned Task Embeddings for Coreference Resolution</strong> 📊 <strong>OpenReview:</strong> N/A | <strong>Suggestion:</strong> Open | 🔗 <a href="https://aclanthology.org/2025.repl4nlp-1.3/">ACL Anthology</a> <small>🏷️ <strong>Tags:</strong> #probing #knowledge-transfer #coreference-resolution #evaluation-methodology</small></p> <p><strong>Notes:</strong> [PLACEHOLDER - Analysis of methodological approach and findings]</p> <h3 id="text-embeddings">Text Embeddings</h3> <p><strong>Prompt Tuning Can Simply Adapt Large Language Models to Text Encoders</strong> 📊 <strong>OpenReview:</strong> N/A | <strong>Suggestion:</strong> Read (If you like embeedings) | 🔗 <a href="https://aclanthology.org/2025.repl4nlp-1.12/">ACL Anthology</a> <small>🏷️ <strong>Tags:</strong> #prompt-tuning #text-encoders #parameter-efficiency #llm-adaptation</small></p> <p><strong>Notes:</strong> [PLACEHOLDER - Technical comparison and observations]</p> <p><strong>Large Language Models Are Overparameterized Text Encoders</strong> 📊 <strong>OpenReview:</strong> N/A | <strong>Suggestion:</strong> Read (If you like embeedings) | 🔗 <a href="https://aclanthology.org/2025.repl4nlp-1.7/">ACL Anthology</a> <small>🏷️ <strong>Tags:</strong> #model-pruning #overparameterization #text-encoders #efficiency</small></p> <p><strong>Notes:</strong> [PLACEHOLDER - Findings about parameter pruning and efficiency questions]</p> <h3 id="alternative-architectures--pre-training-objectives">Alternative Architectures &amp; Pre-training Objectives</h3> <p><strong>State Space Models are Strong Text Rerankers</strong> 📊 <strong>OpenReview:</strong> N/A | <strong>Suggestion:</strong> Skim | 🔗 <a href="https://aclanthology.org/2025.repl4nlp-1.13/">ACL Anthology</a> <small>🏷️ <strong>Tags:</strong> #state-space-models #mamba #text-reranking #information-retrieval</small></p> <p><strong>Notes:</strong> [PLACEHOLDER - Analysis of experimental dimensions and architecture comparisons]</p> <p><strong>Punctuation Restoration Improves Structure Understanding without Supervision</strong> 📊 <strong>OpenReview:</strong> N/A | <strong>Suggestion:</strong> Skim | 🔗 <a href="https://aclanthology.org/2025.repl4nlp-1.14/">ACL Anthology</a> <small>🏷️ <strong>Tags:</strong> #punctuation-restoration #structural-understanding #unsupervised-learning #linguistic-structure</small></p> <p><strong>Notes:</strong> [PLACEHOLDER - Discussion of core idea and experimental results]</p> <p><strong>DEPTH: Discourse Education through Pre-Training Hierarchically</strong> 📊 <strong>OpenReview:</strong> N/A | <strong>Suggestion:</strong> <a href="#depth-discourse-education-through-pre-training-hierarchically">Read (Standout)</a> | 🔗 <a href="https://aclanthology.org/2025.repl4nlp-1.10/">ACL Anthology</a> <small>🏷️ <strong>Tags:</strong> #discourse-learning #hierarchical-pretraining #educational-nlp</small></p> <h3 id="efficiency-gains">Efficiency Gains</h3> <p><strong>Choose Your Words Wisely: Domain-adaptive Masking Makes Language Models Learn Faster</strong> 📊 <strong>OpenReview:</strong> N/A | <strong>Suggestion:</strong> Open | 🔗 <a href="https://aclanthology.org/2025.repl4nlp-1.5/">ACL Anthology</a> <small>🏷️ <strong>Tags:</strong> #domain-adaptation #efficient-training #masked-language-modeling #biomedical-nlp</small></p> <p><strong>Amuro &amp; Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models</strong> 📊 <strong>OpenReview:</strong> N/A | <strong>Suggestion:</strong> Open | 🔗 <a href="https://aclanthology.org/2025.repl4nlp-1.4/">ACL Anthology</a> <small>🏷️ <strong>Tags:</strong> #pre-training #fine-tuning #continual-learning #model-analysis</small></p> <p><strong>Vocabulary-level Memory Efficiency for Language Model Fine-tuning</strong> 📊 <strong>OpenReview:</strong> N/A | <strong>Suggestion:</strong> Open | 🔗 <a href="https://aclanthology.org/2025.repl4nlp-1.8/">ACL Anthology</a> <small>🏷️ <strong>Tags:</strong> #memory-efficiency #vocabulary-optimization #fine-tuning #resource-optimization</small></p> <h3 id="multi-modal-or-task-specific">Multi-Modal or Task-specific</h3> <p><strong>Cross-Modal Learning for Music-to-Music-Video Description Generation</strong> 📊 <strong>OpenReview:</strong> N/A | <strong>Suggestion:</strong> Open | 🔗 <a href="https://aclanthology.org/2025.repl4nlp-1.11/">ACL Anthology</a> <small>🏷️ <strong>Tags:</strong> #cross-modal #music-video #multimodal-learning #generation</small></p> <p><strong>Efficient Document-level Event Relation Extraction</strong> 📊 <strong>OpenReview:</strong> N/A | <strong>Suggestion:</strong> Open | 🔗 <a href="https://aclanthology.org/2025.repl4nlp-1.9/">ACL Anthology</a> <small>🏷️ <strong>Tags:</strong> #event-extraction #efficiency #document-level #two-stage-framework</small></p> <p><strong>Investigating Adapters for Parameter-efficient Low-resource Automatic Speech Recognition</strong> 📊 <strong>OpenReview:</strong> N/A | <strong>Suggestion:</strong> Open | 🔗 <a href="https://aclanthology.org/2025.repl4nlp-1.6/">ACL Anthology</a> <small>🏷️ <strong>Tags:</strong> #adapters #parameter-efficiency #speech-recognition #low-resource</small></p> <h2 id="standout-papers">Standout Papers</h2> <h3 id="tracking-universal-features-through-fine-tuning-and-model-merging">Tracking Universal Features Through Fine-Tuning and Model Merging</h3> <p><em>Niels Nielsen Horn, Desmond Elliott</em></p> <h4 id="my-thoughts">My thoughts</h4> <p>[PLACEHOLDER - Analysis of SAE application and mechanistic interpretability implications]</p> <h3 id="depth-discourse-education-through-pre-trained-hierarchically">DEPTH: Discourse Education through Pre-Trained Hierarchically</h3> <p><em>Zachary Elisha Bamberger, Ofek Glick, Chaim Baskin, Yonatan Belinkov</em></p> <h4 id="my-thoughts-1">My thoughts</h4> <p>[PLACEHOLDER - Technical analysis of discourse understanding approach and findings]</p> <h3 id="a-comparative-study-of-learning-paradigms-in-large-language-models-via-intrinsic-dimension">A Comparative Study of Learning Paradigms in Large Language Models via Intrinsic Dimension</h3> <p><em>Saahith Janapati, Yangfeng Ji</em></p> <h4 id="my-thoughts-2">My thoughts</h4> <p>[PLACEHOLDER - Discussion of intrinsic dimension concept and learning paradigm comparisons]</p> <h3 id="bonus-from-tokens-to-thoughts---how-llms-and-humans-trade-compression-for-meaning">Bonus: From Tokens to Thoughts - How LLMs and Humans Trade Compression for Meaning</h3> <p><em>Chen Shani, Dan Jurafsky, Yann LeCun, Ravid Shwartz-Ziv</em></p> <p>[PLACEHOLDER - Detailed analysis of Rate-Distortion Theory application and experimental findings]</p> <h4 id="my-thoughts-3">My thoughts</h4> <p>[PLACEHOLDER - Commentary on Information Bottleneck theory application and implications]</p> <h2 id="next-steps">Next Steps</h2> <h2 id="caveats-and-limitations">Caveats and Limitations</h2>]]></content><author><name></name></author><category term="nlp"/><category term="Research"/><category term="NLP"/><summary type="html"><![CDATA[Why I Care About Representation Learning]]></summary></entry><entry><title type="html">What is Homomorphic Encryption? – OpenMined</title><link href="https://antoniolopardo.github.io/blog/2020/what-is-homomorphic-encryption-openmined/" rel="alternate" type="text/html" title="What is Homomorphic Encryption? – OpenMined"/><published>2020-08-13T00:00:00+00:00</published><updated>2020-08-13T00:00:00+00:00</updated><id>https://antoniolopardo.github.io/blog/2020/what-is-homomorphic-encryption--openmined</id><content type="html" xml:base="https://antoniolopardo.github.io/blog/2020/what-is-homomorphic-encryption-openmined/"><![CDATA[<p>5 years agoThis post is part of our Privacy-Preserving Data Science, Explained series.Check out the companion video to this article on youtube.Input privacy is one of the most relevant issues in private ML. We explored one solution to this problem in Secure Multi-Party Computation. However, if secret sharing was not an option due to the limited number of participants, what’s the alternative? Homomorphic encryption(HE) is a kind of encryption that allows computation on encrypted data. In short, HE ensures that performing operations on encrypted data and decrypting the result is equivalent to performing analogous operations without any encryption. So like SMPC, we can use HE to achieve input privacy but with only one party needed to encrypt and decrypt the data.HE is not only used to protect data owners; model owners have some of the same privacy concerns around their valuable intellectual property (IP) as clients do around their data. Therefore it is crucial when using a model in an untrustworthy environment, to keep its parameters encrypted.Homomorphic encryption has numerous applications that range from healthcare to smart electric grids; from education to machine learning as a service (MLaaS). All sectors where input privacy is paramount and making use of the data is usually already complex due to: regulations, the significance of the data, and security concerns. Other notable uses of the technology involve non-intrusive, privacy-preserving security, i.e., systems capable of detecting nefarious activity from an encrypted and private data source. A useful metaphor for these systems is to think of them as the sniffer dogs of digital data sources. They don’t infringe upon anyone’s privacy thanks to the encryption, their accuracy can be empirically verified, and since their parameters are kept private, they are not easily reverse engineered. Here’s a link to Andrew Trask’s article on privacy-preserving security if you want to dive deeper.PySyft supports the CKKS leveled homomorphic encryption scheme and the Paillier partially homomorphic encryption scheme which is limited to addition but is much faster.More details on CKKS and Paillier are available below in the “theory behind the implementation” section. Here we’ll focus on how to use HE in PySyft.After importing and hooking torch with syft’s additional functionalities we generate the private and public keys. Using the public key we can encrypt syft tensors and with the +  operator we can homomorphically sum them. Intuitively, the private key allows us to decrypt the result of our operations which we can easily print.To use the more complex and powerful CKKS scheme we can follow similar steps. Besides importing syft and torch for CKKS we also use functions from syft.frameworks.tenseal which integrates the TenSEAL package for performing HE operations on tensors.Since 1978, when the idea of Fully Homomorphic Encryption (FHE) was first formalized, several cryptosystems have been invented to get closer to computing arbitrary functions on ciphertext. However, until Craig Gentry introduced the crucial technique of bootstrapping in ’09, they were all partially or somewhat homomorphic encryption schemes. To understand why that is the case, it’s important to define more precisely what “computing arbitrary functions” means.At the level of bits and logic gates, successive combinations of AND and XOR can express any boolean function. Since these two gates behave respectively as operations of binary multiplication and addition we can conclude that a scheme under which we can perform homomorphic addition and multiplication on ciphertext should be capable of achieving FHE, with some caveats.The operations of addition and multiplication suggest the use of a ring as the underlying algebraic structure. A set of objects, like integers, is considered a ring if we can perform on it an “addition-like” and a “multiplication-like” operation and if it admits neutral elements for both operations, respectively 0 and 1. Examples of rings are, as we have said, the set of all integers but also the set of all possible remainders of an integer division by a specific n, known as “all integers modulo n”. For instance the set of integers modulo 5 Z_5 = {0, 1, 2, 3, 4}, is a ring because we have both 0 and 1 and the addition-like operation that after summing two numbers takes as its result the remainder of the integer division by 5 of that sum. So 3+4 = 7 -&gt; 7%5 = 2. Multiplication works in the same way. 4<em>4 = 16 -&gt; 16%5 = 1. As we’ll detail later we can also build rings with polynomials.Besides homomorphic addition and multiplication what are the other characteristics common to most FHE schemes?They tend to be based on schemes that are capable of “somewhat” homomorphic encryption. These schemes can only perform a limited number of successive multiplication and addition operations on ciphertext before the results become unreliable and impossible to decrypt. This limitation arises directly from the way these systems guarantee security by relying on noise or error to make relatively simple problems computationally intractable. Using multiplication we can operate in a similar way because C(m1)</em>C(m2) = m1m2 + 2(r2m1+r1m2+r1r2) + p(m1q2+2r1q2+q1m2+2q1r2+q1q2)is a valid encryption of m1<em>m2. Here the noise grows much faster than in addition.Before talking about why a growing noise term is such a crucial issue let’s focus on why we need it in the first place. If we were to remove the noise from our scheme it would still be capable of performing homomorphic addition and multiplication. However, to break the noise-less scheme an attacker would just need to get a hold of two encrypted messages and proceed by simply calculating the greatest common divisor for which there is the Euclidean algorithm that runs linearly with the number of digits in the smallest input.If we add noise, the problem becomes much more difficult. In the literature it is known as the Approximate Greatest Common Divisor (GCD) or Approximate Common Divisor (ACD) problem and with reasonable parameters it is considered hard to solve.Approximate GCD is not the only problem used to secure FHE, in fact numerous recent schemes employ the Learning With Errors problem which is also conjectured to be hard to solve.In its most basic formulation the problem states that, given a random uniform integer matrix A and a very small integer vector error e, if we multiply A by a secret vector s and then sum e, recovering s from the resulting vector b without e is computationally intractable even if knowing the matrix A. Similarly to the scheme we explained before, As + e can be thought of as an encryption of 0 and, from that starting point, a different scheme can be developed as presented by prof. Daniele Micciancio in this talk with intuitive addition and multiplication.Even schemes that rely on LWE however, we see the noise rise as successive additions are performed and even more so with multiplications. To understand why that is a problem we’ll go back to our first scheme with an example.Let’s say, to keep things simple, that we would like to add to itself a number three times Cp(0) =  0 + 2 * 5 + 68 * 29 = 1982 the number is m=0 encrypted with secret key p=29.Cp(0)+Cp(0)+Cp(0) = 5946 = 0 + 2</em>(15) + (204)*29 as always 5946 doesn’t tell us much about the result of our calculation but with p we can decrypt it as we detailed above. We start with 5946%29 = 1 and then we check whether the result is even or odd, 1  is odd so we conclude 3m=1 but of course we know that is not the case so what went wrong?The error term has gotten too big and instead of being even it’s now odd. Since 30 is bigger than p=29 the modulo p operation that usually doesn’t effect the error because it’s too small has now changed it, corrupting the encryption. This example is specific to our scheme and the noise was a little too high to begin with. The behavior of all SWHE schemes become unpredictable as the error grows larger.So is there a way to decrease the error? Yes, bootstrapping is a technique that involves running the decryption procedure homomorphically without revealing the message and using the encrypted secret key.To give a sense of how this is possible we should keep in mind that we can produce a series of XOR and AND gates or additions and multiplications that perform the decryption operation. In our scheme, the modulo p followed by modulo 2. Furthermore, the scheme we presented here encrypts binary messages. To get an encryption of the secret key we simply need to have an ordered set of encryptions of its bits.With these two ingredients we can perform homomorphic decryption and eliminate the noise produced by previous operations. Homomorphic decryption however injects some noise of its own, like any other function. As long as we can still perform reliably one operation of addition or of multiplication before needing bootstrapping again we have reached FHE. This is the recipe for most proposed FHE schemes, an underlying SWHE scheme that supports addition and multiplication, usually secured by adding noise, and a way to reduce the noise when it grows too large, usually by bootstrapping.The following are some examples of HE schemes: some partial, others fully homomorphic. I have provided short descriptions and links to resources to understand them better.In 1999 Pascal Paillier invented a partially homomorphic, asymmetric cryptosystem now bearing his last name. Paillier’s scheme is homomorphic with respect to addition. For more details on the actual algorithm take a look at the article dedicated to it on our blog.In short, it achieves HE with respect to addition by encrypting the message as an exponent of the public key. This way when multiplying two ciphertexts encrypted with the same key the result is a valid encryption of the sum.Original paper: Public-Key Cryptosystems Based on Composite Degree Residuosity ClassesVideo resource: Implementation of Homomorphic Encryption: PaillierCKKS has been developed by researchers at Seoul National University and UC San Diego and it is characterized by using the approximate nature of floating-point arithmetic as part of the LHE scheme.OpenMined uses CKKS as the primary way to encrypt tensors on which we want to perform both addition and multiplication.OpenMined demo on CKKS: Homomorphic Encryption in PySyft with SEAL and PyTorch Original paper: Homomorphic Encryption for Arithmetic of Approximate NumbersVideo resource: Introduction to CKKS (Approximate Homomorphic Encryption)It uses rings over polynomials and has an approachable construction similar to the scheme we described in this post but using LWE. It was developed by Brakerski,Fan and Vercauteren. We have a beginner friendly post on how to implement it in Python.OM Implementation: Build an Homomorphic Encryption Scheme from Scratch with PythonGreat Blogpost: A Homomorphic Encryption Illustrated Primer Original Paper: Somewhat Practical Fully Homomorphic EncryptionGSW uses LWE applied to linear algebra where the messages are encrypted as eigenvalues of matrices which have a common eigenvector. GSW was developed by Craig Gentry, Amit Sahai, and Brent Waters.Original Paper: Homomorphic Encryption from Learning with Errors:Conceptually-Simpler, Asymptotically-Faster, Attribute-BasedVideo resource: Fully Homomorphic EncryptionBGV can use modulus switching, an alternative technique for noise management. BGV was developed by Zvika Brakerski,Craig Gentry and Vinod Vaikunathan.Original paper: Fully Homomorphic Encryption without BootstrappingSEAL is an open-source library developed by Microsoft that implements the BFV and CKKS schemes in both their symmetric and asymmetric versions. The library is written in C++ but has many wrappers for Python and JavaScript.HElib supports BGV and CKKS with a focus on ciphertext “packing” techniques that increase the efficiency of the base schemes. To describe the library approach to HE the developers have written that in HElib “The  underlying  cryptosystem  serves as  the  equivalent  of  a “hardware  platform”,  in  that  it defines  a  set  of   operations  that can  be  applied  homomorphically, and  specifies   their  cost.” HElib has been developed in C++ by researchers at IBM and it is open-source.Python-paillier open-source implementation in python of the Paillier scheme.TFHE implements HE at the binary gate level with a ring-variant of the GSW scheme and applies gate-by-gate bootstrapping. cuFHE is the cuda enabled version of TFHE. The library is open-source and written in C.Palisade is an open-source library supported by DARPA that implements the BFV, BGV, CKKS, TFHE, FHEW schemes and provides other useful features to support lattice-based cryptography.You might also be interested in: Homomorphic Encryption in PySyft with SEAL and PyTorch, Build an Homomorphic Encryption Scheme from Scratch with Python, What is the Paillier Cryptosystem?Sign up to recieve an email when new content like this is posted.Want to write for OpenMined or help update a post?Let us know!By sending, you agree to our privacy policyand join the OpenMined Newsletter.January 8, 2025November 7, 2024Follow usLearn MoreSolutionsOur vision for the future is ambitious. Here is how you can help:© 2025 OpenMined FoundationOpenMined is a 501(c)(3) non-profit foundation and a global community on a mission to create the public network for non-public information.With your support, we can unlock the world’s insights while making privacy accessible to everyone.We can do it, with your help.Secure Donation</p>]]></content><author><name></name></author><category term="OpenMined"/><summary type="html"><![CDATA[This post is part of our Privacy-Preserving Data Science, Explained series.]]></summary></entry><entry><title type="html">What is Federated Learning? – OpenMined</title><link href="https://antoniolopardo.github.io/blog/2020/what-is-federated-learning-openmined/" rel="alternate" type="text/html" title="What is Federated Learning? – OpenMined"/><published>2020-05-19T00:00:00+00:00</published><updated>2020-05-19T00:00:00+00:00</updated><id>https://antoniolopardo.github.io/blog/2020/what-is-federated-learning--openmined</id><content type="html" xml:base="https://antoniolopardo.github.io/blog/2020/what-is-federated-learning-openmined/"><![CDATA[<p>5 years agoThis post is part of our Privacy-Preserving Data Science, Explained series. Update as of November 18, 2021: The version of PySyft mentioned in this post has been deprecated. Any implementations using this older version of PySyft are unlikely to work. Stay tuned for the release of PySyft 0.6.0, a data centric library for use in production targeted for release in early December.In this article of the introductory series on Private ML, we will introduce Federated Learning (FL), explaining what FL is, when to use it, and how to implement it with OpenMined tools. The information in this article will be digestible for a broad audience, but section by section, we will go more into the weeds to understand and use Federated Learning.For more info about the series, check out the intro article or take a look at the other posts to learn more about the techniques that can enable privacy-preserving ML with OpenMined’s libraries.Initially proposed in 2015, federated learning is an algorithmic solution that enables the training of ML models by sending copies of a model to the place where data resides  and performing training at the edge, thereby eliminating the necessity to move large amounts of data to a central server for training purposes.The data remains at its source devices, a.k.a. the clients, which receive a copy of the global model from the central server. This copy of the global model is trained locally with  the data of each device. The model weights are updated via local training, and then the local copy is  sent back to the central server. Once the server receives the updated model, it proceeds to aggregate the updates, improving the global model without revealing any of the private data on which it was trained.One of the first applications of FL was to improve word recommendation in Google’s Android keyboard without uploading the data, i.e. a user’s text, to the cloud. More recently, Apple has detailed how it employs federated learning to improve Siri‘s voice recognition. Besides, intuitively, keeping the data at its source is valuable in any privacy-preserving applications, especially when applied in healthcare or on confidential data in business and government.To get started we will use the classical MNIST data set that will stand in for our clients’ data, PySyft will provide all the components needed to demo federated learning and test it locally on this data set. If you want to imagine a reasonably close application, we could conceive that the MNIST characters are part of the digital signatures of our clients, produced when signing documents on their smartphone and we would like to use them to train a character recognition model. In this scenario we would like to provide strong privacy assurances to our users by not uploading their signatures to a central server. In PySyft, the clients’ devices, that is, the entities performing model training, are called workers.At this point in or mock example we have to send the data to the workers using PySyft’s Federated Data Loader.The Federated Data Loader, like the standard torch data loader on which is based, enables lazy loading during training and so in the train function, because the data batches are now distributed across Alice and bob, you need to send the model to the right location for each batch using model.send(…). Then you perform all the operations remotely with the same syntax as if you were doing everything locally on PyTorch. When you’re done, you get back the model updated and the loss that can be logged using the .get() method.At this point, the model has been trained on the data of both Alice and Bob by the respective workers, and their data has not left their devices.Federated learning alone, however, is not enough to ensure privacy because using the model updates, an “honest-but-curious” server could reconstruct the samples from which the updates were computed. This is where Secure MultiParty Computation, homomorphic encryption, and differential privacy come to provide stronger guarantees of security to data owners. We will explore all these topics in this series. As we mentioned above the larger vision for the technology goes beyond any single application or service. With the help of federated learning data owners can more easily maintain control of their data that can be used to train models without leaving the owners’ systems. These guarantees, besides being positive for all users of data-intensive applications, have the potential to make available whole new data sets in sectors like healthcare where, to follow HIPPA or the health related provisions of GDPR, privacy is the top priority. To contribute to making this vision a reality OpenMined is working on PyGrid a peer-to-peer platform that uses the PySyft framework for Federated Learning and data science. Data owners and data scientists can connect on the platform, where the data owners can feel safe in the knowledge that their data will never leave their node, and data scientists can perform their analysis without infringing on anyone’s privacy rights. Today, this type of interaction could take from weeks to months in sectors working on sensitive data, but with PyGrid it could all be just a few lines of code away. To learn more about PyGrid, here is a deeper dive in the platform and the use cases it enables. OpenMined would like to thank Antonio Lopardo, Emma Bluemke, Théo Ryffel, Nahua Kang, Andrew Trask, Jonathan Lebensold, Ayoub Benaissa, and Madhura Joshi, Shaistha Fathima, Nate Solon, Robin Röhm, Sabrina Steinert, Michael Höh and Ben Szymkow for their contributions to various parts of this series.Sign up to recieve an email when new content like this is posted.Want to write for OpenMined or help update a post?Let us know!By sending, you agree to our privacy policyand join the OpenMined Newsletter.January 8, 2025November 7, 2024Follow usLearn MoreSolutionsOur vision for the future is ambitious. Here is how you can help:© 2025 OpenMined FoundationOpenMined is a 501(c)(3) non-profit foundation and a global community on a mission to create the public network for non-public information.With your support, we can unlock the world’s insights while making privacy accessible to everyone.We can do it, with your help.Secure Donation</p>]]></content><author><name></name></author><category term="OpenMined"/><summary type="html"><![CDATA[This post is part of our Privacy-Preserving Data Science, Explained series. Update as of November 18, 2021: The version of PySyft mentioned in this post has been deprecated. Any implementations using this older version of PySyft are unlikely to work. Stay tuned for the release of PySyft 0.6.0,]]></summary></entry><entry><title type="html">What is Secure Multi-Party Computation? – OpenMined</title><link href="https://antoniolopardo.github.io/blog/2020/what-is-secure-multi-party-computation-openmined/" rel="alternate" type="text/html" title="What is Secure Multi-Party Computation? – OpenMined"/><published>2020-05-19T00:00:00+00:00</published><updated>2020-05-19T00:00:00+00:00</updated><id>https://antoniolopardo.github.io/blog/2020/what-is-secure-multi-party-computation--openmined</id><content type="html" xml:base="https://antoniolopardo.github.io/blog/2020/what-is-secure-multi-party-computation-openmined/"><![CDATA[<p>5 years agoThis post is part of our Privacy-Preserving Data Science, Explained series. As we mentioned in one of the previous posts in this series, federated learning is not enough to develop privacy-preserving ML applications. In fact, to keep the model from merely copying what it receives, the data needs to be kept secret while still permitting training and inference. One way to achieve this objective, with both significant advantages and trade-offs, is secret sharing in secure multi-party computation. Today we’ll explore secure multi-party computation (SMPC) and explore how it can help us achieve input privacy. Similar to the post on FL, we hope that all the information in this article will be digestible for a broad audience, but section by section, we will go more into the weeds to understand and use this technique. For more info about the series, check out the intro article or take a look at the other posts to learn more about the technologies that can enable privacy-preserving ML with OpenMined’s libraries.Broadly speaking, SMPC techniques are ways for parties to compute a function jointly while keeping their inputs secret. In the case of ML, this function might be a model’s loss function during training, or it could be the model itself in inference.SMPC tends to have a significant communication overhead but has the advantage that, unless a substantial proportion of the parties are malicious and coordinating, the input data will remain private even if sought after for unlimited time and resources. Secret sharing in SMPC can protect both models’ parameters and training/inference data.Machine Learning as a Service is one of the most significant use-cases of SMPC as it would allow companies to offer their models to perform inference on private data sent by their clients, while ensuring the utmost privacy. For example, in the medical field, a cloud service provider could run a trained classification model on secretly shared patient data and send the secretly shared result (e.g., a prediction of a disease) back to the patient.Other notable applications of the technology involve non-intrusive, privacy-preserving security, i.e. systems capable of detecting nefarious activity from an encrypted and private data source. A useful metaphor for these systems is to think of them as the sniffer dogs of digital data sources. They don’t infringe upon anyone’s privacy thanks to the protection provided by secret sharing, their accuracy can be empirically verified, and since their parameters are kept private, they shouldn’t be easily reverse engineered. For more info on this use case, check-out Andrew Trask’s great blog post that goes more in-depth on similar applications using Homomorphic encryption to protect the data, secret sharing in SMPC can be used in much the same way.PySyft implements secret sharing and fixed precision encoding, we’ll detail both below but in PySyft they are two very simple tensor methods.One of the easiest to understand implementations of secret sharing in SMPC is additive secret sharing, as explained in the Udacity course on Private AI. Additive secret sharing boils down to the idea that, a number let’s say x=5, can be split in several shares let’s say two,  share_1=2 and share_2=3, managed independently by two participants, let’s call them Alice and Candice. At this point, if we were to apply any number of addition operations on the shares individually and then sum the results, this sum would be the same as applying those same additions and on x=5 . To facilitate the encoding of negative numbers and increase the security of the protocol, we use the modulo operation with a very large prime number. The addition operations could also be performed with other encrypted numbers by adding up the shares of each of the addend. We can also perform multiplication between one encrypted number and a non encrypted number, by viewing the the operation as a series of additions. For multiplication between encrypted numbers PySyft implements the SPDZ protocol that is an extension of additive secret sharing, encrypt, decrypt and add are the same, but it enables more complex operations than addition. Operations like multiplication where SPDZ manages to maintain the encrypted numbers private during the computation by using a triple of numbers generated by a crypto provider that is not otherwise involved in the computation. In the code at the beginning of the implementation section the crypto provider is secure_worker. Taking a closer look at the operations we can see that since alpha * beta == xy - xb - ay + ab,  b * alpha == bx - ab, and a * beta == ay - ab if we add them all together and then sum a*b we will effectively return a privately shared version ofxy. For a more in-depth look at SPDZ and how it implements other functions check out these blog posts by Morten Dahl.This schema appears quite simple, but it already permits all operations, as combinations of additions and multiplications, between two secretly shared numbers. Indeed, like the more complex Homomorphic encryption schemes that work with a single party, SPDZ allows computation on ciphertexts generating an encrypted result which, when decrypted, matches the result of the operations as if they had been performed on the plaintext. In this case, splitting the data into shares is the encryption, adding the shares back together is the decryption, while the shares are the ciphertext on which to operate.This technique is adequate for integers, covering the encryption of things like the values of the pixels in images or the counts of entries in a database. The parameters of many ML models like neural networks, however, are floats, so how can we use additive secret sharing in ML?  We need to introduce a new ingredient, Fixed Precision Encoding, an intuitive technique that enables computation to be performed on floats encoded in integers values. In base 10 the encoding is as simple as removing the decimal point while keeping as many decimal places as indicated by the precision.SMPC is also one of the pillars of PyGrid, OpenMined’s peer-to-peer platform that uses the PySyft framework for Federated Learning and data science. The platform uses secure multiparty computation in cases when the overhead in communication is manageable, for example, when using a model only for inference. In those cases, this technique protects both data and model’s parameters and enables the kind of Private MLaaS applications that we introduced in this article.OpenMined would like to thank Antonio Lopardo, Emma Bluemke, Théo Ryffel, Nahua Kang, Andrew Trask, Jonathan Lebensold, Ayoub Benaissa, and Madhura Joshi, Shaistha Fathima, Nate Solon, Robin Röhm, Sabrina Steinert, Michael Höh and Ben Szymkow for their contributions to various parts of this series.Sign up to recieve an email when new content like this is posted.Want to write for OpenMined or help update a post?Let us know!By sending, you agree to our privacy policyand join the OpenMined Newsletter.January 8, 2025November 7, 2024Follow usLearn MoreSolutionsOur vision for the future is ambitious. Here is how you can help:© 2025 OpenMined FoundationOpenMined is a 501(c)(3) non-profit foundation and a global community on a mission to create the public network for non-public information.With your support, we can unlock the world’s insights while making privacy accessible to everyone.We can do it, with your help.Secure Donation</p>]]></content><author><name></name></author><category term="OpenMined"/><summary type="html"><![CDATA[This post is part of our Privacy-Preserving Data Science, Explained Simply series.]]></summary></entry><entry><title type="html">BART: Are all pretraining techniques created equal? | by Antonio Lopardo | DAIR.AI | Medium</title><link href="https://antoniolopardo.github.io/blog/2020/bart-are-all-pretraining-techniques-created-equal-by-antonio-lopardo-dairai-medium/" rel="alternate" type="text/html" title="BART: Are all pretraining techniques created equal? | by Antonio Lopardo | DAIR.AI | Medium"/><published>2020-05-15T00:00:00+00:00</published><updated>2020-05-15T00:00:00+00:00</updated><id>https://antoniolopardo.github.io/blog/2020/bart-are-all-pretraining-techniques-created-equal--by-antonio-lopardo--dairai--medium</id><content type="html" xml:base="https://antoniolopardo.github.io/blog/2020/bart-are-all-pretraining-techniques-created-equal-by-antonio-lopardo-dairai-medium/"><![CDATA[<p>Sign upSign inSign upSign inDemocratizing Artificial Intelligence Research, Education, Technologies–ListenShareIn this paper, Lewis et al. present valuable comparative work on different pre-training techniques and show how this kind of work can be used to guide large pre-training experiments reaching state-of-the-art (SOTA) results.The authors propose a framework to compare pre-training techniques and language model (LM) objectives. This framework focuses on how these techniques can be viewed as corrupting text with an arbitrary noising function while the language model is tasked with denoising it. After some comparative experiments using this framework, BART is introduced as a transformer-based LM that reaches SOTA performance.The idea behind the proposed framework is simple, they suggest that decoupling language models and the functions with which the texts are corrupted are useful to compare different pre-training techniques and see how they perform on similar models and diverse benchmarks. Viewed this way, pre-training is a sequence of repeated steps:In the first experiment, using the framework introduced at the beginning of the article, the authors compared different pre-training techniques and LM objectives on a smaller than usual model, BART-base. The model uses a 6 layered, transformer-based, seq2seq architecture for autoencoding as introduced by Vaswani et al. The pre-training techniques compared in the experiments can be divided between those that work at the token level and those that work at the sentence level:Intuitively, the techniques that work at the sentence level should help the LM learn the different roles of sentences in a paragraph or longer text and in the process help dealing with natural language generation (NLG) tasks.Besides the pre-training techniques, the authors also compare different LM objectives focusing on the ones used by BERT and GPT as well as techniques that tried to incorporate the best of both worlds:From the results of these first experiments, the authors draw some important conclusions.Token masking is crucialOnly the configurations with token masking or its variations achieve consistently great performance on different tasks.Left-to-right pre-training improves NLGThe Classical Language Model objective despite not doing well in inference or question answering tasks achieves SOTA on ELI5(Explain Like I’m 5).Bidirectional encoders are crucial for QAIgnoring future context hinders the performance of left-to-right models.While pre-training techniques and LM objectives are important, the authors make note of the fact that they do not provide the full picture. They report that their permuted language model performs much worse than XLNet because BART lacks some of the valuable architectural innovations introduced in XLNet.After the comparative experiment, the authors trained a 12 layered, transformer-based architecture for autoencoding, and using similar hyperparameters to RoBERTa. They used both a form of token masking at 30% and sentence permutation as pre-training text-noising techniques and run the model on 160GB of news, books, stories, and web text, similar to what’s done in RoBERTa.BART performs best in abstractive summarization tasks especially in the XSum benchmark that contains very few examples of summaries where phrases are present both in the summary and the original text. Besides surpassing the previous best systems in summarization by a considerable margin, BART does well also in natural language inference (NLI) tasks and QA, where it is on par with SOTA results.The paper also features examples of summaries of WikiNews articles produced by a version of BART fine-tuned on the XSum dataset:From these examples, BART appears capable of producing coherent grammatical sentences that capture the sense of the text it should summarize. It highlights names and places why ignoring other details like dates and figures.If you want to summarize some text of your own we have set up a Google Colab notebook using the Hugging Face library.—-Democratizing Artificial Intelligence Research, Education, TechnologiesMSc of computer science student at Politecnico di Milano. Data Science and NLP practitioner, writer at OpenMined.HelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech</p>]]></content><author><name></name></author><category term="NLP"/><category term="Research"/><summary type="html"><![CDATA[In this paper, Lewis et al. present valuable comparative work on different pre-training techniques and show how this kind of work can be used to guide large pre-training experiments reaching…]]></summary></entry><entry><title type="html">Word2Vec to Transformers. The evolution of word embeddings, notes… | by Antonio Lopardo | TDS Archive | Medium</title><link href="https://antoniolopardo.github.io/blog/2020/word2vec-to-transformers-the-evolution-of-word-embeddings-notes-by-antonio-lopardo-tds-archive-medium/" rel="alternate" type="text/html" title="Word2Vec to Transformers. The evolution of word embeddings, notes… | by Antonio Lopardo | TDS Archive | Medium"/><published>2020-01-07T00:00:00+00:00</published><updated>2020-01-07T00:00:00+00:00</updated><id>https://antoniolopardo.github.io/blog/2020/word2vec-to-transformers-the-evolution-of-word-embeddings-notes--by-antonio-lopardo--tds-archive--medium</id><content type="html" xml:base="https://antoniolopardo.github.io/blog/2020/word2vec-to-transformers-the-evolution-of-word-embeddings-notes-by-antonio-lopardo-tds-archive-medium/"><![CDATA[<p>Sign upSign inSign upSign inAn archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former Towards Data Science Medium publication.–ListenShareDeveloping meaningful representations of words has been one of the primary goals of NLP since its inception. This foundational task has been, during the 2010s, one of the main drivers of advances and innovation in the field. In this post I’ll present some of the main approaches to this task and the major ideas that greatly improved our ability to capture word senses and similarity in different contexts.The simplest approach to the problem is called Bag of Words. This approach assigns a unique token, usually a number, to each word that appears in the text. So for example the phrase “Your argument is sound, nothing but sound” would be represented as “1-2-3-4-5-6-4” . With this baseline approach we can capture a notion of identity between words, that is we recognize when a word is used more than one time. Furthermore, with techniques like Tf-Idf which uses Bag of Words, we can measure, with some degree of success, similarity between documents just based on which words are used in the documents and with which frequency. Using resources like WordNet, a supped up dictionary, we might also discover the multiple senses of the words in our text and connect the ones that are listed as synonyms in the dictionary. But the representation itself doesn’t capture word similarity or the specific sense in which the word has been used in our text.Arguably the most development in NLP in the early 2010s has been Word2Vec an unsupervised learning technique to learn continuous representations of words. In many ways Word2Vec builds on BoW but instead of assigning discrete tokens to words it learns continuous multi-dimensional vector representation for each word in the training corpus. More specifically it does so by learning to predict given a center word the most likely words in a fixed sized window around it (Skip-Gram).Or by learning to predict the center word based on the context words (Continuous Bag of Words). Like many other machine learning techniques Word2Vec uses gradient descent to minimize over the entire corpus the cross-entropy loss, that is the probability of predicting the wrong word. Indeed the basic idea is to maximize at any given configuration of center word and outside word the conditional probability of predicting the outside word given the center word.Now if we wanted to maximize this expression over all the corpus we would need to progressively learn vectors that better capture the similarity between words. Therefore Word2Vec by design captures the similarity of words in our corpus and thank to the notion of how far or near a word is in vector space to other words a notion of word-sense.This notion of similarity however only takes us so far. The main problem of Word2Vec is that it provides a single representation for a word that is the same regardless of context. So words like “bank” with several different senses, for example river bank and investment bank, will end up with a representation which is an average of the senses not representing either one well.Proving more than one representation for each word based on the context in which it appears is the core idea behind contextual word embeddings. This idea is implemented using an RNN language model trained similarly to Word2Vec in an unsupervised manner more specifically we use RNNs to predict, given a current word in a sentence, the next word. We use these networks for their ability to capture and maintain long term dependencies in their hidden states.The idea is that after feeding the current word we can concatenate the hidden state to the usual Word2Vec representation to maintain both information on the current word and the past context.One of the first systems to implement this idea was TagLM by Peters and co.TagLM used a pre-trained Bi-LSTM language model to produce the “contextual part” of the word embedding that gets concatenated to a Word2Vec vector or more complex character level CNN/RNN generated representation of the word. This representation is now the new embedding effectively replacing Word2Vec or GloVe vectors in the NLP pipeline.The ELMo embeddings work very similarly, the main difference is that ELMo uses a two layer Bi-LSTM for the pre-trained language model and the embedding to concatenate is a learnable, during fine-tuning, combination of the two layers to be optimize for the specific task. ELMo also ditches Word2Vec completely by relying only on character level CNN/RNNs for the first part of the word representation.The idea of training a separate language model to produce better contextual word representation has proved very successful in improving the SOTA in many NLP tasks but RNN language models due to their recurrent, sequential nature tend to be slow to train and very hard to parallelize. So in 2017 Aswani and colleagues developed a non recurrent alternative to RNNs at the heart of which there is the Transformer block.The main feature of the Transformer is that it uses attention, the concept that helps with alignment in seq2seq architectures for translation, to capture relationships between the words of a sentence similarly to how convolution do it. And just like convolution we can make multiple attention heads to calculate for each word where it should focus its attention and the relationship that the attention represents. Attention is however different from convolution in the sense that it captures similarity between the words in the space in which the weight matrices for the different heads project their representations. In order to capture more distant dependencies similarly to convolutions we can stack multiple transformers blocks.The decoder block works somewhat differently but the images for the Attention is all you need paper make everything clearer.BERT uses the transformer block to train a language model using a masking technique where the system isn’t tasked with guessing the next word but rather one of the words masked out in the sentence.This way it is able to use the entire context for prediction and not only the left context.These notes are intended as a very bare-bones summary of lectures 13 and 14 from the CS224n Stanford class. Credits for the material belong to the professor, Chris Manning, and TAs of the course.More material is available at the course’s site and on youtube.—-An archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former Towards Data Science Medium publication.MSc of computer science student at Politecnico di Milano. Data Science and NLP practitioner, writer at OpenMined.HelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech</p>]]></content><author><name></name></author><category term="NLP"/><summary type="html"><![CDATA[Developing meaningful representations of words has been one of the primary goals of NLP since its inception. This foundational task has been, during the 2010s, one of the main drivers of advances and…]]></summary></entry><entry><title type="html">The basics of Language Modeling. Notes from CS224n lesson 6 and 7. | by Antonio Lopardo | Medium</title><link href="https://antoniolopardo.github.io/blog/2019/the-basics-of-language-modeling-notes-from-cs224n-lesson-6-and-7-by-antonio-lopardo-medium/" rel="alternate" type="text/html" title="The basics of Language Modeling. Notes from CS224n lesson 6 and 7. | by Antonio Lopardo | Medium"/><published>2019-12-23T00:00:00+00:00</published><updated>2019-12-23T00:00:00+00:00</updated><id>https://antoniolopardo.github.io/blog/2019/the-basics-of-language-modeling-notes-from-cs224n-lesson-6-and-7--by-antonio-lopardo--medium</id><content type="html" xml:base="https://antoniolopardo.github.io/blog/2019/the-basics-of-language-modeling-notes-from-cs224n-lesson-6-and-7-by-antonio-lopardo-medium/"><![CDATA[<p>Sign upSign inSign upSign in–ListenShareLanguage modeling is one of the benchmark tasks of NLP. In its simplest form, it consists of predicting the most probable word following a series of words based on them.There are many applications of this task, namely, google autocomplete and word suggestions in most modern mobile keyboards.The systems used to perform this task tend to learn from an extensive corpus and thus follow a supervised learning approach.Intuitively, in general, more common words like “cat ”or “dog ”should tend to have higher probabilities then more uncommon ones such as aardvark or kingfisher. Thus, a good starting off point could be the frequency of words in the corpus. A system for this purpose that takes into account only the number of appearances of a word normalized by the number of words in the corpus is called a uni-gram language model. Similarly, bi-gram language models consider the frequency of couples of word, for example, if in our English corpus the couple [united, states] appears more often than [united, the] a bi-gram language model would assign a higher probability to “states ” rather then to “the” to follow “united ”despite the much higher frequency of the latter. Higher-gram language models also exist, but as the dimensions of the sequences of words increase, their frequency in the corpus decreases exponentially. These models thus have a sparsity problem and struggle with infrequent word sequences.Word embedding can help solve the sparsity problem since with can ditch the one-hot representation of words for their vectors. Therefore we can easily build neural language models that can take into consideration more words than the n-gram models. Despite this advantage, however, the number of words considered remains fixed, and so neural language models still struggle with long-term dependencies.To increase the context available to our language model, we can use a recurrent neural network, a type of stateful NN. The network for each word maintains a state influenced by both the current word and previous hidden state. This architecture removes the fixes size of the context alleviating the problems of simpler neural language models.However, due to the way we calculate gradients the loss at step, each step is influenced considerably by the step just before it and very little by past steps because those gradients tend to become exponentially small as the distance from the step increases.One of the architecture proposed to solve the vanishing gradient problem is called Long Short-Term Memory, and it works by having both and hidden state and a memory cell and three gates the form read (in the hidden state), write and delete of the cell. They are called the output, input, and forget gate, respectively. This architecture implements a dedicated way to maintain long-term dependencies, which is never forgetting the memory cell.These notes are intended as a very barebones summary of lectures six and seven from the CS224n Staford class. Credits for the material belong to the professor, Chris Manning, and TAs of the course.More material is available at the course’s site and on youtube.—-MSc of computer science student at Politecnico di Milano. Data Science and NLP practitioner, writer at OpenMined.HelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech</p>]]></content><author><name></name></author><category term="NLP"/><summary type="html"><![CDATA[Language modeling is one of the benchmark tasks of NLP. In its simplest form, it consists of predicting the most probable word following a series of words based on them. There are many applications…]]></summary></entry><entry><title type="html">The MidTerms on Twitter. I | by Antonio Lopardo | Medium</title><link href="https://antoniolopardo.github.io/blog/2018/the-midterms-on-twitter-i-by-antonio-lopardo-medium/" rel="alternate" type="text/html" title="The MidTerms on Twitter. I | by Antonio Lopardo | Medium"/><published>2018-11-05T00:00:00+00:00</published><updated>2018-11-05T00:00:00+00:00</updated><id>https://antoniolopardo.github.io/blog/2018/the-midterms-on-twitter-i--by-antonio-lopardo--medium</id><content type="html" xml:base="https://antoniolopardo.github.io/blog/2018/the-midterms-on-twitter-i-by-antonio-lopardo-medium/"><![CDATA[<p>Sign upSign inSign upSign in–ListenSharePolitical support inference from twitter has been among the most widely studied areas of NLP (Natural Language Processing) on the platform with many valid approaches developed over the years since its wide adoption. A number of these approaches rely on sentiment analysis, classification of tweets between positive and negative, to measure the support of parties and leaders, others put an emphasis on classifying users rather than tweets to model users as voters and not the tweets themselves as indicators of support but not many of them measure up to the task of analyzing contentious national parliamentary elections where both local and national leaders, parties and issues are at play. The system proposed here aims to tackle one such election, the US 2018 House of Representative election, but the approach followed in its development, while requiring considerable domain knowledge, is completely language agnostic, i.e. none of the tools, techniques and data acquisition strategies used are language exclusive. The standout features of the system are: its reliance on regional data and the parallel yet distinct analysis of local support for national figures and factions on one side and of the congressional candidates actually running for office on the other. The methods applied to both analyses rely however on a relativity complex pipeline which will be explained in this paper. The review of this pipeline will be divided in three main areas plus validation.SearchHow the system queries for geotagged tweets and tweets containing the candidates’ name or handle.AnalyzeHow the NLP models perform, how they are trained and how the training and testing data was acquired.VisualizeHow the data after analysis is displayed on map and how to make available as much information as possible while avoiding clutter and confusion.<em>Bonus</em> Full system “validation”How to validate the system using districts where the outcome is forecasted with an high degree of confidence by reputable sources.The searching methodology for tweets is sometimes overlooked in this kind of analysis as it poses mainly technical rather than theoretical challenges, however as it may in itself introduce biases difficult to detect in later stages of the process it is worth a thorough discussion. As explained before the system is capable of gauging both local support for national causes and the popularity of the candidates running but it does so separately analyzing different data gathered in different ways. This approach mirrors a longstanding practice of US polling where the voters interviewed are queried both on the favorability of the candidates on the ballot and the likelihood that they would support a generic candidate from one party rather than the other. The two methodologies will respectively be referred to as “On Ballot”(OB) and “Generic Ballot”(GB). Again borrowing from US polling practice the number of districts on which the analysis will focus is relatively small. Indeed the races for many congressional seats, due to factors like incumbent’s advantage, are not very competitive. Therefore consulting reputable sources such as the Cook Political Report it’s possible to cut down on the number of districts to analyze from 435, the seats in house, to about 68 the races rated lean or toss up as of Oct 10th.Query composition for On Ballot tweetsThe source of data for both approaches is the twitter standard search API but if in GB, as will be explained later, much emphasis is put on geo-tagged local data with OB no regional limitation is put on the query. In fact the queries for OB contain only the specific candidate’s name and surname and his/her twitter handle e.g.It is reasonable to query in this way on the assumption that most congressional candidate don’t receive national attention and that the vast majority of the twitter activity around the specific election will stem from local voters. Furthermore the aforementioned twitter activity is on the whole sufficient to make analysis since in every district for each candidate an average of about 7k tweets are found over the span of a week.Query composition for Generic Ballot tweetsWhen it comes to GB tweets the search queries complicate. As with OB the standard twitter search API is used this time however the queries contain national leaders and parties and are limited to a certain region, the specific congressional district queried. For example this is the query for tweets on Republicans.In order to limit the search to the congressional district the geocode field in the twitter API search query is used. The field however takes only a tuple for coordinates of the center of the search circle and a float for its radius. But as very few congressional districts may be approximated well with a single circle more queries are needed for a single district to cover its area. The approximation may not always be exactly fitted to the district’s shape but guided by population density this method is able to best suit the use case and analysis.When it comes to the actual analysis of the tweets gathered both GB and OB use very similar RNN-LSTM binary classifiers trained to identify democratic or republican leaning tweets fed by a Word2Vec embedding layer itself trained on tweets. This architecture has been proven accurate and adaptable in a number of NLP tasks, namely sentiment analysis, and trained on enough tweets, hundreds of thousands, it is capable of reaching satisfactory accuracy of classification in this use case. The architecture of the models is however a less important variable as compared to the quality and quantity of the training and testing data so before a more thorough exploration of the models an explanation of the techniques and methods used to gather this data is in order.Training data acquisition for On Ballot and Generic Ballot modelsThe models use different training data gathered in very similar ways. The main technique employed consists in downloading the entire twitter timeline, as far back as about 5–6 months ago, of users with clear political affiliation. Among these are the accounts of the politicians themselves, of partisan political pundits, of the most well-known activists and involved organizers, e.g. @KamalaHarris @davidhogg111 @billmaher for democrats and @tedcruz @TuckerCarlson @DonaldJTrumpJr for republicans. The accounts selected for this election yielded around 160k tweets that based on the unequivocal political leaning of the users writing them were consequently labeled as democrats (0) or as republicans (1). Training the RNNs only with this data however was not enough to reach accuracy beyond the high sixties on the testing set. In order to improve on this result by increasing the variety of the data, 120k other tweets were added to the training set. This second batch differed from the first as these tweets belong to lesser know users of the platform who nevertheless are much more well versed in the vernacular of daily political interactions on twitter e.g. @StormResist @deejay90192 (D) and @Robfortrump2020 @qanon76 (R). Adding this new data allowed the GB model to surpass 70% accuracy and to settle in the low to mid seventies on the testing set. Furthermore by adding a confidence threshold of 75% the accuracy rises to nearly 80% at the cost of around one quarter of the tweets that are discarded as they don’t pass the threshold. The OB model aside from the training set used for GB was trained on a further 140k tweets coming from the congressional candidates themselves to make the model more aware of the vocabulary of day-in day-out campaigning. Despite this additional training the OB model across the board does not match he accuracy of the GB model because of the similarities in the lexicon used by candidates of both sides. “Turn out the vote” tweets and tweets containing poll information are especially difficult to classify.Testing data acquisition and labeling for Generic and On Ballot modelsThe sole reliable testing for true validation of an NLP model like these ones is evaluation on hand-labeled real world data. The queries described in the search section were used to acquire the tweets and the testing set was formed maintaining balance between the republican and democratic leaning tweets and between the tweets that contained republicans or democrats.Training, testing and exploration of Generic Ballot and On Ballot modelsNo tweet of the training set reaches the model before pre-processing and tokenization. Pre-processing entails the removal of unwanted characters, stop words and making the tweets as uniform as possible by, for example, lowering all uppercase letters.Tokenization is a more complicated process, indeed as the RNN cannot directly learn on text the tweets need to be converted to sequences of integers and the role of the tokenizer is keeping track of the correspondence between number and words. However not every word is worth keeping, as it might just be a name or handle unnecessary for the analysis, therefore only the most frequent 15000 words are tokenized while the others are discarded. 15000 is a relatively high number as compared the 3–5k typical of sentiment analysis models but this abundance is key to capturing the more nuanced nature of political opinions. Another important parameter in the tokenization process is maximum length in number of words allowed for a tweet, as a matter of fact every tweet when tokenized is converted to a sequence of integers of fixed length, the ones that don’t contatins enough words are padded with 0s at then end and the ones that are too long are either discarded or cut short. Therefore the maxlen allowed should perform an important balancing act between capturing complex messages in long tweets while keeping to a minimum padding of short tweets as this process may hinder training speed and effectiveness.After pre-processing and tokenization the training set reaches the Word2Vec embedding layer which feeds the RNN. The 128 wide embedding layer is used to decrease the complexity of incoming tweets by learning abstract representation of every word to facilitate training of the network.The RNN itself is composed of two layers of which the exact dimensions can be respectivly between 128 and 256 and between 32 and 64. A number of different techniques are used to improve training such as dropout, which limits the risk that the network might start to rely always on the same kind of signals, or the use of a second layer, which helps with abstractions in certain situations where the literal meaning of the tweet might be a red herring. However as mentioned before the exact details of the RNN don’t have as high an impact on accuracy as the quality of the training data. And the accuracy metric is only as good as the testing set from which it comes so is worth taking an anecdotal look at which tweets of the testing set the model labels correctly and which ones it fails to classify.The network does not have any problems with tweets containing clear partisan markers e.g. #MAGA or #MarchForOurLives.The confidence lowers on tweets with more ambiguous meaning.The model is able to classify also snarky or satirical tweets.More policy dense tweets are caught too.Where the system stumbles though is on quotes of contrasting opinions or referrals to other statements.Looking at these examples one might be tempted to assume the network actually understands political opinions but the system works only on a syntactic level, the same opinion expressed in very uncommon language might be classified differently. To test the merits and limitations of the model for yourself there’s a demo.Nevertheless as stated in the intro none of the steps and techniques used to train, test and validate the models are language specific or domain specific, as long as there are users with clear leaning and which tweet enough about the subject no opinion extraction task is off limits.<em>Bonus</em> Full System “Validation”No surefire way exists for validating aggregate system like this one, aside from the election itself, of course. It is however possible to get a sense of the effectiveness of the system by trying to classify the polar opposites of the spectrum i.e. districts that reputable sources such as the aforementioned Cook Political Report or fivethirtyeight.com rate as LIKELY/SAFE. Districts therefore where the outcome can be forcasted with confidence. Thus a testing sets of sorts can be compiled on for example 10 such districts. Validating the system on a set of this kind yielded satisfactory results as all ten districts where classified correctly.The product of the system is a geojson file with the favorability score of each side saved in its fields. At this point there are a number of different ways to display the map. Coloring the map in solid blue or red to show which districts lean for the democrats and which for the republicans similarly to the visualization done on the last Italian parliamentary election of 2018 available at http://www.twitterpoliticalsentiment.com/Italia/Camera/ is an option.However as the colors to display in this case are only two a good way to enrich the map would be to color the districts on a spectrum between blue and red to visualize not only which party is more likely to win the seat but also the degree of confidence of the prediction.The general applicability of the principles and methods of this kind of analysis forms the basis a more complex project which attempts to provide a user friendly integrated way to setup, carry out and understand regional twitter political alignment analysis. The project in its very early, incomplete, much less than MVP state is available at TwitterSentMapper.—-MSc of computer science student at Politecnico di Milano. Data Science and NLP practitioner, writer at OpenMined.HelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech</p>]]></content><author><name></name></author><category term="NLP"/><category term="Research"/><summary type="html"><![CDATA[Political support inference from twitter has been among the most widely studied areas of NLP (Natural Language Processing) on the platform with many valid approaches developed over the years since…]]></summary></entry></feed>